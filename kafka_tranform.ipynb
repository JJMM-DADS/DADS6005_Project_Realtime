{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dd8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_HOME\"] ='C:/Users/kan_2/spark-2.4.8-bin-hadoop2.7'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.8 pyspark-shell'\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars C:/Users/kan_2/spark-streaming-kafka-0-8-assembly_2.11-2.4.8.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e348aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb05ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6e76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Normalizer, StandardScaler\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "from confluent_kafka import Producer\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26d372",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "ssc = StreamingContext(sc,6) # Show results every 2 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247786b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_topic_name = \"jjmm_binance\"\n",
    "kafka_topic_name2 = \"jjmm_bitkub\"\n",
    "kafka_topic_name3 = \"jjmm_exchange\"\n",
    "kafka_bootstrap_servers = 'ec2-13-229-46-113.ap-southeast-1.compute.amazonaws.com:9092'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d877c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delivery_report(err, msg):\n",
    "    \"\"\" Called once for each message produced to indicate delivery result.\n",
    "        Triggered by poll() or flush(). \"\"\"\n",
    "    if err is not None:\n",
    "        print('Message delivery failed: {}'.format(err))\n",
    "    else:\n",
    "        print('Message delivered to {}'.format(msg.value().decode('utf-8')))\n",
    "\n",
    " #file1.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_kafka(msg):\n",
    "    ID=uuid.uuid1()\n",
    "    p = Producer({'bootstrap.servers': 'ec2-13-229-46-113.ap-southeast-1.compute.amazonaws.com:9092'})\n",
    "    ID=uuid.uuid1()\n",
    "    p.poll(0)\n",
    "    if type(msg)!=str:\n",
    "        msg=str(msg)\n",
    "    sendMsg = msg.encode().decode('utf-8').strip('\\n')\n",
    "    p.produce('jjmm-stream', key=\"id{}\".format(ID.hex), value=sendMsg , callback=delivery_report)\n",
    "    p.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45adc963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMyStream(rdd):\n",
    "  if not rdd.isEmpty():\n",
    "    df = spark.read.json(rdd)\n",
    "    print(df.dtypes)\n",
    "    print('Started the Process')\n",
    "    print('Selection of Columns')\n",
    "    df = df.select('firstname','lastname')\n",
    "    df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccedf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMyStream2(rdd):\n",
    "  if not rdd.isEmpty():\n",
    "    df = spark.read.json(rdd)\n",
    "    #df.write.mode('Overwrite').json(\"/Users/kan_2/spark_output/zipcodes\")\n",
    "    #df=resultDF.to_json()\n",
    "    #df.write.mode('append').json(\"C:/6005/test.json\")\n",
    "#    df2 = spark.read.json(rdd2)\n",
    "\n",
    "    # The inferred schema can be visualized using the printSchema() method\n",
    "    df.printSchema()\n",
    "#    df2.printSchema()\n",
    "    # Creates a temporary view using the DataFrame\n",
    "    df.createOrReplaceTempView(\"T1\")\n",
    "\n",
    "    # SQL statements can be run by using the sql methods provided by spark\n",
    "    resultDF = spark.sql(\"SELECT * FROM T1\")\n",
    "    resultDF = resultDF.toPandas()\n",
    "    result_csv = resultDF['timestamp'].to_list()\n",
    "    a=result_csv[0].replace(\"/\",\"-\")\n",
    "    b=a.replace(\":\",\"-\")\n",
    "    resultDF.to_csv(\"C:/Users/kan_2/testoutput\"+b+\".csv\",index=False)\n",
    "    c=resultDF.to_dict('records')[0]\n",
    "    produce_kafka(c)\n",
    "    \n",
    "    #final_df.append(resultDF)\n",
    "    #resultDF.show()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdd63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMyStream3(rdd1,rdd2):\n",
    "  if not rdd1.isEmpty() and not rdd2.isEmpty():\n",
    "    df1 = spark.read.json(rdd1)\n",
    "    #df1.printSchema()\n",
    "    df2 = spark.read.json(rdd2)\n",
    "    #df2.printSchema()\n",
    "    #df3 = spark.read.json(rdd3)\n",
    "    #df3.printSchema()\n",
    "    \n",
    "    df1.createOrReplaceTempView(\"binance\")\n",
    "    df2.createOrReplaceTempView(\"exchange\")\n",
    "    #df3.createOrReplaceTempView(\"exchange\")\n",
    "\n",
    "    df1 = df1.sort(desc('timestamp'))\n",
    "    df2 = df2.sort(desc('timestamp'))\n",
    "    #df3 = df3.sort(desc('timestamp'))\n",
    "    \n",
    "    #df1.show()\n",
    "    #df2.show()\n",
    "    A = spark.sql(\"SELECT timestamp,priceChange,openPrice,highPrice,lowPrice,lastPrice FROM binance limit 1\")\n",
    "    B = spark.sql(\"SELECT rate,symbol FROM exchange limit 1\")\n",
    "    A_pd = A.toPandas()\n",
    "    B_pd = B.toPandas()\n",
    "    out = pd.concat([A_pd,B_pd],axis=1)\n",
    "    out['last_price(THB)'] = out['lastPrice']*out['rate']\n",
    "    out['priceChange(THB)'] = out['priceChange']*out['rate']\n",
    "    out['openPrice(THB)'] = out['openPrice']*out['rate']\n",
    "    out['highPrice(THB)'] = out['highPrice']*out['rate']\n",
    "    out['lowPrice(THB)'] = out['lowPrice']*out['rate']\n",
    "    #out.to_csv(\"C:/Users/kan_2/testoutput2.csv\",index=False)\n",
    "    c=out.to_dict('records')[0]\n",
    "    produce_kafka(c)\n",
    "    #df3.show()\n",
    "    \n",
    "    #resultDF = df1.join(df2, df1.id == df2.id, 'outer').select(df2.firstname, df1.hobby)#.sort(desc(\"name\")).collect()\n",
    "    #resultDF.show()\n",
    "    # SQL statements can be run by using the sql methods provided by spark\n",
    "    #resultDF = spark.sql(\"SELECT * FROM people\")\n",
    "    #resultDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd32eaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kvs1 = KafkaUtils.createDirectStream(ssc, \n",
    "                                    [kafka_topic_name], \n",
    "                                    {\n",
    "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
    "                        'group.id':'test-group',\n",
    "                        'auto.offset.reset':'largest'\n",
    "                        })\n",
    "'''\n",
    "kvs2 = KafkaUtils.createDirectStream(ssc, \n",
    "                                    [kafka_topic_name2], \n",
    "                                    {\n",
    "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
    "                        'group.id':'test-group',\n",
    "                        'auto.offset.reset':'largest'\n",
    "                        })\n",
    "'''\n",
    "kvs3 = KafkaUtils.createDirectStream(ssc, \n",
    "                                    [kafka_topic_name3], \n",
    "                                    {\n",
    "                        'bootstrap.servers':kafka_bootstrap_servers,\n",
    "                        'group.id':'test-group',\n",
    "                        'auto.offset.reset':'largest'\n",
    "                        })\n",
    "                       \n",
    "#kvs1.pprint()\n",
    "#kvs2.pprint()\n",
    "#kvs3.pprint()\n",
    "parsed1 = kvs1.map(lambda x: x[1])\n",
    "#parsed2 = kvs2.map(lambda x: x[1])\n",
    "parsed3 = kvs3.map(lambda x: x[1])\n",
    "parsed1.transformWith(readMyStream3,parsed3).pprint()\n",
    "#a = parsed1.foreachRDD( lambda rdd: readMyStream2(rdd) )\n",
    "#b = parsed2.transform( lambda rdd: readMyStream2(rdd) ).pprint()\n",
    "\n",
    "\n",
    "\n",
    "#wordCounts.transform(lambda rdd: rdd.join(spamInfoRDD).filter(...))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "ssc.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbbe92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
